{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_RJ3lkC-8UxE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: nltk in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: openpyxl in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/trinitydhillon/miniconda3/envs/caricon/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if missing (run in notebook)\n",
    "%pip install pandas scikit-learn nltk openpyxl\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 2B tested imports\n",
    "from sklearn.decomposition import TruncatedSVD  # avoid overfitting: working on spare data TF-IDF\n",
    "from sklearn.preprocessing import StandardScaler # normalization ensures one feature doesn’t dominate others\n",
    "from sklearn.pipeline import Pipeline # chain multiple model steps into a single workflow: cross-validation with one obj \n",
    "from sklearn.linear_model import LogisticRegression # test how well TF-IDF + structure predict the MBTI\n",
    "from sklearn.model_selection import cross_val_score # Evaluate model via CV\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "NLMCfKfflm2Z",
    "outputId": "218bca0c-8f4c-4110-e912-e9ab5fbc8f12"
   },
   "outputs": [],
   "source": [
    "# # Original import:\n",
    "\n",
    "# # mbti dataset\n",
    "# mbti = pd.read_csv(\"/content/myers-briggs-data.csv\")\n",
    "# mbtis = pd.read_csv(\"/content/myer-briggs-data.cvs.csv\")\n",
    "\n",
    "# # ONET datasets\n",
    "# onet_salaries = pd.read_csv(\"/content/onet_data_with_salaries.csv\")\n",
    "# onet_humn = pd.read_csv(\"/content/onet_data_with_human_characteristics.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# MBTI dataset (CSV)\n",
    "mbti = pd.read_csv(\"dataset/myer-briggs-data.csv\")\n",
    "#mbtis = pd.read_csv(\"/content/myer-briggs-data.cvs.csv\")\n",
    "\n",
    "# ONET datasets (Excel files, so use read_excel)\n",
    "#onet_salaries = pd.read_excel(\"dataset/onet_data_with_salaries.xlsx\")\n",
    "onet_humn = pd.read_excel(\"dataset/onet_data_with_human_characteristics.xlsx\")\n",
    "\n",
    "#Added:\n",
    "onet_complete = pd.read_excel(\"dataset/complete_onet_data_with_human_characteristics.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "BIpSjgnrmEG8",
    "outputId": "ae6803b6-165f-466e-9b3e-0e7d0e6cc816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n=====MBTI Dataset INSPECTION=====\n",
      "Shape: (8675, 2)\n",
      "Columns: Index(['type', 'posts'], dtype='object')\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n",
      "None\n",
      "\n",
      "Head:\n",
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n",
      "\n",
      "Missing Values per Column:\n",
      "type     0\n",
      "posts    0\n",
      "dtype: int64\n",
      "\n",
      "Summary Statistics:(numeric columns):\n",
      "        type                                              posts\n",
      "count   8675                                               8675\n",
      "unique    16                                               8675\n",
      "top     INFP  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "freq    1832                                                  1\n",
      "\n",
      "Outlier Check (based on IQR, numeric columns):\n",
      "==================================================\n",
      "/n=====O*NET Human Characteristics Dataset INSPECTION=====\n",
      "Shape: (1016, 30)\n",
      "Columns: Index(['Occupation Code', 'Title', 'Description', 'Sample Job Titles', 'Tasks',\n",
      "       'Knowledge', 'Skills', 'Abilities', 'Work Activities',\n",
      "       'Detailed Work Activities', 'Tools Used', 'Technology Used', 'Job Zone',\n",
      "       'Education Level', 'Experience Required', 'Job Training', 'SVP Range',\n",
      "       'Job Zone Examples', 'Interests', 'Work Styles', 'Work Values',\n",
      "       'Work Context', 'Additional Sources', 'Related Occupations',\n",
      "       'Annual 10th Percentile', 'Annual 25th Percentile',\n",
      "       'Annual Median Wage', 'Annual 75th Percentile',\n",
      "       'Annual 90th Percentile', 'Human Characteristics'],\n",
      "      dtype='object')\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1016 entries, 0 to 1015\n",
      "Data columns (total 30 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Occupation Code           1016 non-null   object \n",
      " 1   Title                     1016 non-null   object \n",
      " 2   Description               1016 non-null   object \n",
      " 3   Sample Job Titles         901 non-null    object \n",
      " 4   Tasks                     923 non-null    object \n",
      " 5   Knowledge                 873 non-null    object \n",
      " 6   Skills                    869 non-null    object \n",
      " 7   Abilities                 878 non-null    object \n",
      " 8   Work Activities           879 non-null    object \n",
      " 9   Detailed Work Activities  923 non-null    object \n",
      " 10  Tools Used                897 non-null    object \n",
      " 11  Technology Used           923 non-null    object \n",
      " 12  Job Zone                  923 non-null    object \n",
      " 13  Education Level           923 non-null    object \n",
      " 14  Experience Required       923 non-null    object \n",
      " 15  Job Training              923 non-null    object \n",
      " 16  SVP Range                 923 non-null    object \n",
      " 17  Job Zone Examples         923 non-null    object \n",
      " 18  Interests                 923 non-null    object \n",
      " 19  Work Styles               879 non-null    object \n",
      " 20  Work Values               874 non-null    object \n",
      " 21  Work Context              879 non-null    object \n",
      " 22  Additional Sources        949 non-null    object \n",
      " 23  Related Occupations       923 non-null    object \n",
      " 24  Annual 10th Percentile    990 non-null    float64\n",
      " 25  Annual 25th Percentile    990 non-null    object \n",
      " 26  Annual Median Wage        990 non-null    object \n",
      " 27  Annual 75th Percentile    990 non-null    object \n",
      " 28  Annual 90th Percentile    990 non-null    object \n",
      " 29  Human Characteristics     1016 non-null   object \n",
      "dtypes: float64(1), object(29)\n",
      "memory usage: 238.2+ KB\n",
      "None\n",
      "\n",
      "Head:\n",
      "  Occupation Code                     Title  \\\n",
      "0      13-2011.00  Accountants and Auditors   \n",
      "1      27-2011.00                    Actors   \n",
      "2      15-2011.00                 Actuaries   \n",
      "3      29-1291.00            Acupuncturists   \n",
      "4      29-1141.01         Acute Care Nurses   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Examine, analyze, and interpret accounting rec...   \n",
      "1  Play parts in stage, television, radio, video,...   \n",
      "2  Analyze statistical data, such as mortality, a...   \n",
      "3  Diagnose, treat, and prevent disorders by stim...   \n",
      "4  Provide advanced nursing care for patients wit...   \n",
      "\n",
      "                                   Sample Job Titles  \\\n",
      "0  Accountant, Accounting Officer, Audit Partner,...   \n",
      "1  Actor, Actress, Comedian, Comic, Community The...   \n",
      "2  Actuarial Analyst, Actuarial Associate, Actuar...   \n",
      "3  Acupuncture Physician, Acupuncture Provider, A...   \n",
      "4  Cardiac Interventional Care Nurse, Charge Nurs...   \n",
      "\n",
      "                                               Tasks  \\\n",
      "0  Prepare detailed reports on audit findings., R...   \n",
      "1  Collaborate with other actors as part of an en...   \n",
      "2  Ascertain premium rates required and cash rese...   \n",
      "3  Develop individual treatment plans and strateg...   \n",
      "4  Perform emergency medical procedures, such as ...   \n",
      "\n",
      "                                           Knowledge  \\\n",
      "0  Economics and Accounting, English Language, Ma...   \n",
      "1  Fine Arts, English Language, Communications an...   \n",
      "2  Mathematics, Computers and Electronics, Econom...   \n",
      "3  Medicine and Dentistry, Customer and Personal ...   \n",
      "4  Medicine and Dentistry, Customer and Personal ...   \n",
      "\n",
      "                                              Skills  \\\n",
      "0  Reading Comprehension, Active Listening, Criti...   \n",
      "1  Reading Comprehension, Speaking, Active Listen...   \n",
      "2  Critical Thinking, Judgment and Decision Makin...   \n",
      "3  Active Listening, Critical Thinking, Social Pe...   \n",
      "4  Active Listening, Critical Thinking, Monitorin...   \n",
      "\n",
      "                                           Abilities  \\\n",
      "0  Oral Comprehension, Oral Expression, Written C...   \n",
      "1  Oral Expression, Oral Comprehension, Memorizat...   \n",
      "2  Mathematical Reasoning, Inductive Reasoning, N...   \n",
      "3  Deductive Reasoning, Oral Comprehension, Oral ...   \n",
      "4  Oral Comprehension, Problem Sensitivity, Deduc...   \n",
      "\n",
      "                                     Work Activities  \\\n",
      "0  Getting Information, Communicating with Superv...   \n",
      "1  Establishing and Maintaining Interpersonal Rel...   \n",
      "2  Analyzing Data or Information, Processing Info...   \n",
      "3  Assisting and Caring for Others, Updating and ...   \n",
      "4  Assisting and Caring for Others, Documenting/R...   \n",
      "\n",
      "                            Detailed Work Activities  ...  \\\n",
      "0  Prepare financial documents, reports, or budge...  ...   \n",
      "1  Collaborate with others to prepare or perform ...  ...   \n",
      "2  Manage financial activities of the organizatio...  ...   \n",
      "3  Follow protocols or regulations for healthcare...  ...   \n",
      "4  Treat medical emergencies., Administer anesthe...  ...   \n",
      "\n",
      "                                     Work Values  \\\n",
      "0         Achievement, Independence, Recognition   \n",
      "1       Relationships, Achievement, Independence   \n",
      "2  Working Conditions, Achievement, Independence   \n",
      "3       Achievement, Independence, Relationships   \n",
      "4            Relationships, Support, Achievement   \n",
      "\n",
      "                                        Work Context  \\\n",
      "0  E-Mail, Telephone Conversations, Face-to-Face ...   \n",
      "1  Work With or Contribute to a Work Group or Tea...   \n",
      "2  E-Mail, Indoors, Environmentally Controlled, S...   \n",
      "3  Indoors, Environmentally Controlled, Physical ...   \n",
      "4  Exposed to Disease or Infections, Telephone Co...   \n",
      "\n",
      "                                  Additional Sources  \\\n",
      "0  AACSB, AICPA and CIMA, American Accounting Ass...   \n",
      "1  Actors' Equity Association, American Associati...   \n",
      "2  American Academy of Actuaries, Casualty Actuar...   \n",
      "3  American Association of Acupuncture and Orient...   \n",
      "4  American Association of Colleges of Nursing, A...   \n",
      "\n",
      "                                 Related Occupations Annual 10th Percentile  \\\n",
      "0  Bookkeeping, Accounting, and Auditing Clerks (...                50440.0   \n",
      "1  Choreographers (27-2032.00), Music Directors a...                    NaN   \n",
      "2  Accountants and Auditors (13-2011.00), Compens...                75380.0   \n",
      "3  Cardiologists (29-1212.00), Chiropractors (29-...                41600.0   \n",
      "4  Advanced Practice Psychiatric Nurses (29-1141....                63720.0   \n",
      "\n",
      "  Annual 25th Percentile Annual Median Wage Annual 75th Percentile  \\\n",
      "0                  62720              79880                 103990   \n",
      "1                    NaN                NaN                    NaN   \n",
      "2                  88420             120000                 164320   \n",
      "3                  52000              78220                  99740   \n",
      "4                  75990              86070                 104670   \n",
      "\n",
      "  Annual 90th Percentile                              Human Characteristics  \n",
      "0                 137280   Detail-oriented, ethical, analytical, dependable  \n",
      "1                    NaN         Expressive, adaptable, creative, resilient  \n",
      "2                 209310  Analytical, detail-oriented, critical thinker,...  \n",
      "3                 140660    Dependable, adaptable, organized, collaborative  \n",
      "4                 132680      Compassionate, patient, attentive, empathetic  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Missing Values per Column:\n",
      "Occupation Code               0\n",
      "Title                         0\n",
      "Description                   0\n",
      "Sample Job Titles           115\n",
      "Tasks                        93\n",
      "Knowledge                   143\n",
      "Skills                      147\n",
      "Abilities                   138\n",
      "Work Activities             137\n",
      "Detailed Work Activities     93\n",
      "Tools Used                  119\n",
      "Technology Used              93\n",
      "Job Zone                     93\n",
      "Education Level              93\n",
      "Experience Required          93\n",
      "Job Training                 93\n",
      "SVP Range                    93\n",
      "Job Zone Examples            93\n",
      "Interests                    93\n",
      "Work Styles                 137\n",
      "Work Values                 142\n",
      "Work Context                137\n",
      "Additional Sources           67\n",
      "Related Occupations          93\n",
      "Annual 10th Percentile       26\n",
      "Annual 25th Percentile       26\n",
      "Annual Median Wage           26\n",
      "Annual 75th Percentile       26\n",
      "Annual 90th Percentile       26\n",
      "Human Characteristics         0\n",
      "dtype: int64\n",
      "\n",
      "Summary Statistics:(numeric columns):\n",
      "       Annual 10th Percentile\n",
      "count              990.000000\n",
      "mean             43523.727273\n",
      "std              17190.357920\n",
      "min              18600.000000\n",
      "25%              31435.000000\n",
      "50%              38350.000000\n",
      "75%              49705.000000\n",
      "max             172810.000000\n",
      "\n",
      "Outlier Check (based on IQR, numeric columns):\n",
      "Annual 10th Percentile: 49 potential outliers\n",
      "==================================================\n",
      "\n",
      " Schema check for O*NET Human Characteristics:\n",
      "  - No missing columns compared to base.\n",
      "  - No extra columns compared to base.\n",
      " The other ONET datasets are redundant once alignment is done.\n",
      "\n",
      " Cleaning pipeline complete.\n",
      " Using `complete_onet_data_with_human_characteristics` as the single source of truth.\n",
      " Final shape: (1016, 30) (rows, columns)\n",
      " Final dataset saved at: dataset/onet_standardized.csv\n",
      "\n",
      " Preview of standardized ONET dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>title</th>\n",
       "      <th>human_characteristics</th>\n",
       "      <th>description</th>\n",
       "      <th>sample_job_titles</th>\n",
       "      <th>tasks</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>skills</th>\n",
       "      <th>abilities</th>\n",
       "      <th>work_activities</th>\n",
       "      <th>...</th>\n",
       "      <th>work_styles</th>\n",
       "      <th>work_values</th>\n",
       "      <th>work_context</th>\n",
       "      <th>additional_sources</th>\n",
       "      <th>related_occupations</th>\n",
       "      <th>annual_10th_percentile</th>\n",
       "      <th>annual_25th_percentile</th>\n",
       "      <th>annual_median_wage</th>\n",
       "      <th>annual_75th_percentile</th>\n",
       "      <th>annual_90th_percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-2011.00</td>\n",
       "      <td>Accountants and Auditors</td>\n",
       "      <td>Attention to Detail, Analytical Thinking, Inte...</td>\n",
       "      <td>Examine, analyze, and interpret accounting rec...</td>\n",
       "      <td>Accountant, Accounting Officer, Audit Partner,...</td>\n",
       "      <td>Prepare detailed reports on audit findings., R...</td>\n",
       "      <td>Economics and Accounting, English Language, Ma...</td>\n",
       "      <td>Reading Comprehension, Active Listening, Criti...</td>\n",
       "      <td>Oral Comprehension, Oral Expression, Written C...</td>\n",
       "      <td>Getting Information, Communicating with Superv...</td>\n",
       "      <td>...</td>\n",
       "      <td>Attention to Detail, Integrity, Dependability,...</td>\n",
       "      <td>Achievement, Independence, Recognition</td>\n",
       "      <td>E-Mail, Telephone Conversations, Face-to-Face ...</td>\n",
       "      <td>AACSB, AICPA and CIMA, American Accounting Ass...</td>\n",
       "      <td>Bookkeeping, Accounting, and Auditing Clerks (...</td>\n",
       "      <td>50440.0</td>\n",
       "      <td>62720</td>\n",
       "      <td>79880</td>\n",
       "      <td>103990</td>\n",
       "      <td>137280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27-2011.00</td>\n",
       "      <td>Actors</td>\n",
       "      <td>Communication, Adaptability, Problem Solving, ...</td>\n",
       "      <td>Play parts in stage, television, radio, video,...</td>\n",
       "      <td>Actor, Actress, Comedian, Comic, Community The...</td>\n",
       "      <td>Collaborate with other actors as part of an en...</td>\n",
       "      <td>Fine Arts, English Language, Communications an...</td>\n",
       "      <td>Reading Comprehension, Speaking, Active Listen...</td>\n",
       "      <td>Oral Expression, Oral Comprehension, Memorizat...</td>\n",
       "      <td>Establishing and Maintaining Interpersonal Rel...</td>\n",
       "      <td>...</td>\n",
       "      <td>Cooperation, Persistence, Adaptability/Flexibi...</td>\n",
       "      <td>Relationships, Achievement, Independence</td>\n",
       "      <td>Work With or Contribute to a Work Group or Tea...</td>\n",
       "      <td>Actors' Equity Association, American Associati...</td>\n",
       "      <td>Choreographers (27-2032.00), Music Directors a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-2011.00</td>\n",
       "      <td>Actuaries</td>\n",
       "      <td>Communication, Adaptability, Problem Solving, ...</td>\n",
       "      <td>Analyze statistical data, such as mortality, a...</td>\n",
       "      <td>Actuarial Analyst, Actuarial Associate, Actuar...</td>\n",
       "      <td>Ascertain premium rates required and cash rese...</td>\n",
       "      <td>Mathematics, Computers and Electronics, Econom...</td>\n",
       "      <td>Critical Thinking, Judgment and Decision Makin...</td>\n",
       "      <td>Mathematical Reasoning, Inductive Reasoning, N...</td>\n",
       "      <td>Analyzing Data or Information, Processing Info...</td>\n",
       "      <td>...</td>\n",
       "      <td>Analytical Thinking, Attention to Detail, Inte...</td>\n",
       "      <td>Working Conditions, Achievement, Independence</td>\n",
       "      <td>E-Mail, Indoors, Environmentally Controlled, S...</td>\n",
       "      <td>American Academy of Actuaries, Casualty Actuar...</td>\n",
       "      <td>Accountants and Auditors (13-2011.00), Compens...</td>\n",
       "      <td>75380.0</td>\n",
       "      <td>88420</td>\n",
       "      <td>120000</td>\n",
       "      <td>164320</td>\n",
       "      <td>209310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29-1291.00</td>\n",
       "      <td>Acupuncturists</td>\n",
       "      <td>Communication, Adaptability, Problem Solving, ...</td>\n",
       "      <td>Diagnose, treat, and prevent disorders by stim...</td>\n",
       "      <td>Acupuncture Physician, Acupuncture Provider, A...</td>\n",
       "      <td>Develop individual treatment plans and strateg...</td>\n",
       "      <td>Medicine and Dentistry, Customer and Personal ...</td>\n",
       "      <td>Active Listening, Critical Thinking, Social Pe...</td>\n",
       "      <td>Deductive Reasoning, Oral Comprehension, Oral ...</td>\n",
       "      <td>Assisting and Caring for Others, Updating and ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Integrity, Dependability, Self-Control, Attent...</td>\n",
       "      <td>Achievement, Independence, Relationships</td>\n",
       "      <td>Indoors, Environmentally Controlled, Physical ...</td>\n",
       "      <td>American Association of Acupuncture and Orient...</td>\n",
       "      <td>Cardiologists (29-1212.00), Chiropractors (29-...</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>52000</td>\n",
       "      <td>78220</td>\n",
       "      <td>99740</td>\n",
       "      <td>140660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29-1141.01</td>\n",
       "      <td>Acute Care Nurses</td>\n",
       "      <td>Compassion, Patience, Attention to Detail, Str...</td>\n",
       "      <td>Provide advanced nursing care for patients wit...</td>\n",
       "      <td>Cardiac Interventional Care Nurse, Charge Nurs...</td>\n",
       "      <td>Perform emergency medical procedures, such as ...</td>\n",
       "      <td>Medicine and Dentistry, Customer and Personal ...</td>\n",
       "      <td>Active Listening, Critical Thinking, Monitorin...</td>\n",
       "      <td>Oral Comprehension, Problem Sensitivity, Deduc...</td>\n",
       "      <td>Assisting and Caring for Others, Documenting/R...</td>\n",
       "      <td>...</td>\n",
       "      <td>Integrity, Stress Tolerance, Attention to Deta...</td>\n",
       "      <td>Relationships, Support, Achievement</td>\n",
       "      <td>Exposed to Disease or Infections, Telephone Co...</td>\n",
       "      <td>American Association of Colleges of Nursing, A...</td>\n",
       "      <td>Advanced Practice Psychiatric Nurses (29-1141....</td>\n",
       "      <td>63720.0</td>\n",
       "      <td>75990</td>\n",
       "      <td>86070</td>\n",
       "      <td>104670</td>\n",
       "      <td>132680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  occupation_code                     title  \\\n",
       "0      13-2011.00  Accountants and Auditors   \n",
       "1      27-2011.00                    Actors   \n",
       "2      15-2011.00                 Actuaries   \n",
       "3      29-1291.00            Acupuncturists   \n",
       "4      29-1141.01         Acute Care Nurses   \n",
       "\n",
       "                               human_characteristics  \\\n",
       "0  Attention to Detail, Analytical Thinking, Inte...   \n",
       "1  Communication, Adaptability, Problem Solving, ...   \n",
       "2  Communication, Adaptability, Problem Solving, ...   \n",
       "3  Communication, Adaptability, Problem Solving, ...   \n",
       "4  Compassion, Patience, Attention to Detail, Str...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Examine, analyze, and interpret accounting rec...   \n",
       "1  Play parts in stage, television, radio, video,...   \n",
       "2  Analyze statistical data, such as mortality, a...   \n",
       "3  Diagnose, treat, and prevent disorders by stim...   \n",
       "4  Provide advanced nursing care for patients wit...   \n",
       "\n",
       "                                   sample_job_titles  \\\n",
       "0  Accountant, Accounting Officer, Audit Partner,...   \n",
       "1  Actor, Actress, Comedian, Comic, Community The...   \n",
       "2  Actuarial Analyst, Actuarial Associate, Actuar...   \n",
       "3  Acupuncture Physician, Acupuncture Provider, A...   \n",
       "4  Cardiac Interventional Care Nurse, Charge Nurs...   \n",
       "\n",
       "                                               tasks  \\\n",
       "0  Prepare detailed reports on audit findings., R...   \n",
       "1  Collaborate with other actors as part of an en...   \n",
       "2  Ascertain premium rates required and cash rese...   \n",
       "3  Develop individual treatment plans and strateg...   \n",
       "4  Perform emergency medical procedures, such as ...   \n",
       "\n",
       "                                           knowledge  \\\n",
       "0  Economics and Accounting, English Language, Ma...   \n",
       "1  Fine Arts, English Language, Communications an...   \n",
       "2  Mathematics, Computers and Electronics, Econom...   \n",
       "3  Medicine and Dentistry, Customer and Personal ...   \n",
       "4  Medicine and Dentistry, Customer and Personal ...   \n",
       "\n",
       "                                              skills  \\\n",
       "0  Reading Comprehension, Active Listening, Criti...   \n",
       "1  Reading Comprehension, Speaking, Active Listen...   \n",
       "2  Critical Thinking, Judgment and Decision Makin...   \n",
       "3  Active Listening, Critical Thinking, Social Pe...   \n",
       "4  Active Listening, Critical Thinking, Monitorin...   \n",
       "\n",
       "                                           abilities  \\\n",
       "0  Oral Comprehension, Oral Expression, Written C...   \n",
       "1  Oral Expression, Oral Comprehension, Memorizat...   \n",
       "2  Mathematical Reasoning, Inductive Reasoning, N...   \n",
       "3  Deductive Reasoning, Oral Comprehension, Oral ...   \n",
       "4  Oral Comprehension, Problem Sensitivity, Deduc...   \n",
       "\n",
       "                                     work_activities  ...  \\\n",
       "0  Getting Information, Communicating with Superv...  ...   \n",
       "1  Establishing and Maintaining Interpersonal Rel...  ...   \n",
       "2  Analyzing Data or Information, Processing Info...  ...   \n",
       "3  Assisting and Caring for Others, Updating and ...  ...   \n",
       "4  Assisting and Caring for Others, Documenting/R...  ...   \n",
       "\n",
       "                                         work_styles  \\\n",
       "0  Attention to Detail, Integrity, Dependability,...   \n",
       "1  Cooperation, Persistence, Adaptability/Flexibi...   \n",
       "2  Analytical Thinking, Attention to Detail, Inte...   \n",
       "3  Integrity, Dependability, Self-Control, Attent...   \n",
       "4  Integrity, Stress Tolerance, Attention to Deta...   \n",
       "\n",
       "                                     work_values  \\\n",
       "0         Achievement, Independence, Recognition   \n",
       "1       Relationships, Achievement, Independence   \n",
       "2  Working Conditions, Achievement, Independence   \n",
       "3       Achievement, Independence, Relationships   \n",
       "4            Relationships, Support, Achievement   \n",
       "\n",
       "                                        work_context  \\\n",
       "0  E-Mail, Telephone Conversations, Face-to-Face ...   \n",
       "1  Work With or Contribute to a Work Group or Tea...   \n",
       "2  E-Mail, Indoors, Environmentally Controlled, S...   \n",
       "3  Indoors, Environmentally Controlled, Physical ...   \n",
       "4  Exposed to Disease or Infections, Telephone Co...   \n",
       "\n",
       "                                  additional_sources  \\\n",
       "0  AACSB, AICPA and CIMA, American Accounting Ass...   \n",
       "1  Actors' Equity Association, American Associati...   \n",
       "2  American Academy of Actuaries, Casualty Actuar...   \n",
       "3  American Association of Acupuncture and Orient...   \n",
       "4  American Association of Colleges of Nursing, A...   \n",
       "\n",
       "                                 related_occupations annual_10th_percentile  \\\n",
       "0  Bookkeeping, Accounting, and Auditing Clerks (...                50440.0   \n",
       "1  Choreographers (27-2032.00), Music Directors a...                    NaN   \n",
       "2  Accountants and Auditors (13-2011.00), Compens...                75380.0   \n",
       "3  Cardiologists (29-1212.00), Chiropractors (29-...                41600.0   \n",
       "4  Advanced Practice Psychiatric Nurses (29-1141....                63720.0   \n",
       "\n",
       "  annual_25th_percentile annual_median_wage annual_75th_percentile  \\\n",
       "0                  62720              79880                 103990   \n",
       "1                    NaN                NaN                    NaN   \n",
       "2                  88420             120000                 164320   \n",
       "3                  52000              78220                  99740   \n",
       "4                  75990              86070                 104670   \n",
       "\n",
       "  annual_90th_percentile  \n",
       "0                 137280  \n",
       "1                    NaN  \n",
       "2                 209310  \n",
       "3                 140660  \n",
       "4                 132680  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###TASK 1: Dataset Inspection###\n",
    "#inspecting the datasets\n",
    "def inspect_dataset(df,name=\"Dataset\"):\n",
    "  print(f\"/n====={name} INSPECTION=====\")\n",
    "\n",
    "#looking through shape and info about the datasets\n",
    "  print(f\"Shape: {df.shape}\")\n",
    "  print(f\"Columns: {df.columns}\")\n",
    "  print(\"Info:\")\n",
    "  print(df.info())\n",
    "\n",
    "  print(\"\\nHead:\")\n",
    "  print(df.head())\n",
    "#missing values\n",
    "  print(\"\\nMissing Values per Column:\")\n",
    "  print(df.isnull().sum())\n",
    "\n",
    "#summary statistics\n",
    "  print(\"\\nSummary Statistics:(numeric columns):\")\n",
    "  print(df.describe())\n",
    "\n",
    "  #for checking outliers\n",
    "  print(\"\\nOutlier Check (based on IQR, numeric columns):\")\n",
    "  numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "  for col in numeric_cols:\n",
    "      q1 = df[col].quantile(0.25)\n",
    "      q3 = df[col].quantile(0.75)\n",
    "      iqr = q3 - q1\n",
    "\n",
    "      outliers = df[(df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))]\n",
    "      print(f\"{col}: {len(outliers)} potential outliers\")\n",
    "\n",
    "  print(\"=\"*50)\n",
    "\n",
    "# Inspecting all the datasets\n",
    "inspect_dataset(mbti, \"MBTI Dataset\")\n",
    "#inspect_dataset(mbtis, \"MBTI (duplicate?) Dataset\")\n",
    "#inspect_dataset(onet_salaries, \"O*NET Salaries Dataset\")\n",
    "inspect_dataset(onet_humn, \"O*NET Human Characteristics Dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###TASK 2: Standardization Diagnostics:###\n",
    "# Step 1: Align Column Names\n",
    "def standardize_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "#onet_salaries = standardize_columns(onet_salaries)\n",
    "onet_humn = standardize_columns(onet_humn)\n",
    "onet_complete = standardize_columns(onet_complete)\n",
    "\n",
    "# Reorder columns to match complete dataset (base schema)\n",
    "#onet_salaries = onet_salaries.reindex(columns=onet_complete.columns.intersection(onet_salaries.columns))\n",
    "onet_humn = onet_humn.reindex(columns=onet_complete.columns)\n",
    "\n",
    "# Step 2: Identify Redundancy\n",
    "def compare_columns(base, other, name):\n",
    "    base_cols = set(base.columns)\n",
    "    other_cols = set(other.columns)\n",
    "\n",
    "    missing = base_cols - other_cols\n",
    "    extra = other_cols - base_cols\n",
    "\n",
    "    print(f\"\\n Schema check for {name}:\")\n",
    "    if missing:\n",
    "        print(f\"  - Missing columns: {missing}\")\n",
    "    else:\n",
    "        print(\"  - No missing columns compared to base.\")\n",
    "    if extra:\n",
    "        print(f\"  - Extra columns: {extra}\")\n",
    "    else:\n",
    "        print(\"  - No extra columns compared to base.\")\n",
    "\n",
    "#compare_columns(onet_complete, onet_salaries, \"O*NET Salaries\")\n",
    "compare_columns(onet_complete, onet_humn, \"O*NET Human Characteristics\")\n",
    "\n",
    "\n",
    "print(\" The other ONET datasets are redundant once alignment is done.\")\n",
    "\n",
    "# Step 3: Drop Duplicates\n",
    "# Keep only the complete dataset as the single source of truth\n",
    "onet_standardized = onet_complete.drop_duplicates()\n",
    "\n",
    "# Save Cleaned Dataset\n",
    "output_path = \"dataset/onet_standardized.csv\"\n",
    "onet_standardized.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n Cleaning pipeline complete.\")\n",
    "print(\" Using `complete_onet_data_with_human_characteristics` as the single source of truth.\")\n",
    "print(f\" Final shape: {onet_standardized.shape} (rows, columns)\")\n",
    "print(f\" Final dataset saved at: {output_path}\")\n",
    "\n",
    "# Preview first rows\n",
    "print(\"\\n Preview of standardized ONET dataset:\")\n",
    "display(onet_standardized.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "r35cLlPH0KZf",
    "outputId": "a165fdec-76f8-44fb-aaf4-56e8b1a51913"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>intj moments sportscenter plays prankswhat lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good course say know thats blessing cursedoes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre firedthats silly misconception approachi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  intj moments sportscenter plays prankswhat lif...  \n",
       "1  im finding lack posts alarmingsex boring posit...  \n",
       "2  good course say know thats blessing cursedoes ...  \n",
       "3  dear intp enjoyed conversation day esoteric ga...  \n",
       "4  youre firedthats silly misconception approachi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TASK 3: Clean data, remove stopwords and tokenize text ###\n",
    "\n",
    "\n",
    "# Prepare stop words (combine sklearn's stop words with NLTK's if available)\n",
    "stop_words = set(ENGLISH_STOP_WORDS)\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    stop_words = stop_words.union(set(nltk_stop.words('english')))\n",
    "except Exception:\n",
    "    # NLTK stopwords may not be downloaded in this environment; it's ok to proceed\n",
    "    pass\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "mbti['cleaned_text'] = mbti.get('posts', '') .apply(clean_text) if 'posts' in mbti.columns else ''\n",
    "\n",
    "# TF-IDF vectorizer (only run if cleaned text present)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=5)\n",
    "if 'cleaned_text' in mbti.columns and mbti['cleaned_text'].astype(bool).any():\n",
    "    X = vectorizer.fit_transform(mbti['cleaned_text'])\n",
    "    y = mbti['type'] if 'type' in mbti.columns else None\n",
    "else:\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "mbti.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2A-Prep complete.\n",
      "Loaded:       type                                              posts  \\\n",
      "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
      "1     ENTP  'I'm finding the lack of me in these posts ver...   \n",
      "2     INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
      "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
      "4     ENTJ  'You're fired.|||That's another silly misconce...   \n",
      "...    ...                                                ...   \n",
      "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
      "8671  ENFP  'So...if this thread already exists someplace ...   \n",
      "8672  INTP  'So many questions when i do these things.  I ...   \n",
      "8673  INFP  'I am very conflicted right now when it comes ...   \n",
      "8674  INFP  'It has been too long since I have been on per...   \n",
      "\n",
      "                                           cleaned_text  \n",
      "0     intj moments sportscenter plays prankswhat lif...  \n",
      "1     im finding lack posts alarmingsex boring posit...  \n",
      "2     good course say know thats blessing cursedoes ...  \n",
      "3     dear intp enjoyed conversation day esoteric ga...  \n",
      "4     youre firedthats silly misconception approachi...  \n",
      "...                                                 ...  \n",
      "8670  just think cats fi doms reason websites haven ...  \n",
      "8671  soif thread exists someplace does heck delete ...  \n",
      "8672  questions things purple pill pick winning lott...  \n",
      "8673  conflicted right comes wanting children honest...  \n",
      "8674  long personalitycafe doesnt changed bit say go...  \n",
      "\n",
      "[8675 rows x 3 columns]  | rows: 8675\n",
      "Saved (16-type one-hot only): dataset/mbti_onehot_only.csv  | shape: (8675, 19)\n",
      "Saved (OHE + axes + continuous): dataset/mbti_encoded_with_axes.csv  | shape: (8675, 31)\n",
      "OHE columns (first 6): ['type_ENFJ', 'type_ENFP', 'type_ENTJ', 'type_ENTP', 'type_ESFJ', 'type_ESFP'] ...\n",
      "Axes columns: ['IE', 'NS', 'TF', 'JP', 'IE_y', 'NS_y', 'TF_y', 'JP_y']\n",
      "Continuous trait columns: ['extraversion_score', 'sensing_score', 'thinking_score', 'judging_score']\n"
     ]
    }
   ],
   "source": [
    "### Task 2A-Prep: Encode MBTI Labels ###\n",
    "\n",
    "\n",
    "# ---------- A) One-Hot Encoding for 16 MBTI types ----------\n",
    "\n",
    "try:\n",
    "    # sklearn >= 1.2 uses 'sparse_output'\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    OHE_KW = dict(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    _ = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "except TypeError:\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    OHE_KW = dict(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "if 'type' not in mbti.columns:\n",
    "    raise ValueError(\"Expected a 'type' column in the MBTI dataset.\")\n",
    "\n",
    "# Fit & transform\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe = encoder.fit_transform(mbti[['type']])\n",
    "\n",
    "# Build DF with aligned index\n",
    "ohe_df = pd.DataFrame(\n",
    "    ohe,\n",
    "    columns=encoder.get_feature_names_out(['type']),\n",
    "    index=mbti.index\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "mbti_ohe = pd.concat([mbti, ohe_df], axis=1)\n",
    "\n",
    "# ---------- B) Binary Axes (I/E, N/S, T/F, J/P) ----------\n",
    "# These are standard for 4 separate binary classifiers\n",
    "def split_axes(mbti_type: str):\n",
    "    t = str(mbti_type).upper()\n",
    "    if len(t) != 4:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan], index=[\"IE\",\"NS\",\"TF\",\"JP\"])\n",
    "    return pd.Series([t[0], t[1], t[2], t[3]], index=[\"IE\",\"NS\",\"TF\",\"JP\"])\n",
    "\n",
    "axes = mbti['type'].apply(split_axes)\n",
    "\n",
    "label_maps = {\n",
    "    \"IE\": {\"I\": 1, \"E\": 0},\n",
    "    \"NS\": {\"N\": 1, \"S\": 0},\n",
    "    \"TF\": {\"T\": 1, \"F\": 0},\n",
    "    \"JP\": {\"J\": 1, \"P\": 0},\n",
    "}\n",
    "for col, mapping in label_maps.items():\n",
    "    axes[col + \"_y\"] = axes[col].map(mapping)\n",
    "\n",
    "mbti_encoded = pd.concat([mbti_ohe, axes], axis=1)\n",
    "\n",
    "# ---------- C) (Optional) Continuous Trait Matrix (0–10) ----------\n",
    "# Keep this only if you plan to use it for visualization or a toy score.\n",
    "def mbti_to_continuous(mbti_type: str):\n",
    "    if not isinstance(mbti_type, str) or len(mbti_type) != 4:\n",
    "        return [np.nan, np.nan, np.nan, np.nan]\n",
    "    return [\n",
    "        10 if mbti_type[0].upper() == 'E' else 0,  # E vs I\n",
    "        10 if mbti_type[1].upper() == 'S' else 0,  # S vs N\n",
    "        10 if mbti_type[2].upper() == 'T' else 0,  # T vs F\n",
    "        10 if mbti_type[3].upper() == 'J' else 0,  # J vs P\n",
    "    ]\n",
    "\n",
    "traits = mbti['type'].apply(lambda t: pd.Series(\n",
    "    mbti_to_continuous(t),\n",
    "    index=['extraversion_score', 'sensing_score', 'thinking_score', 'judging_score']\n",
    "))\n",
    "mbti_encoded = pd.concat([mbti_encoded, traits], axis=1)\n",
    "\n",
    "# ---------- D) Save Artifacts ----------\n",
    "out_dir = \"dataset\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 1) One-hot only (for pure 16-class multiclass use)\n",
    "out_mbti_ohe = os.path.join(out_dir, \"mbti_onehot_only.csv\")\n",
    "mbti_ohe.to_csv(out_mbti_ohe, index=False)\n",
    "\n",
    "# 2) One-hot + axes + (optional) continuous scores\n",
    "out_mbti_full = os.path.join(out_dir, \"mbti_encoded_with_axes.csv\")\n",
    "mbti_encoded.to_csv(out_mbti_full, index=False)\n",
    "\n",
    "# ---------- E) Console Summary ----------\n",
    "print(\"Task 2A-Prep complete.\")\n",
    "print(f\"Loaded: {mbti}  | rows: {len(mbti)}\")\n",
    "print(f\"Saved (16-type one-hot only): {out_mbti_ohe}  | shape: {mbti_ohe.shape}\")\n",
    "print(f\"Saved (OHE + axes + continuous): {out_mbti_full}  | shape: {mbti_encoded.shape}\")\n",
    "print(\"OHE columns (first 6):\", list(ohe_df.columns)[:6], \"...\")\n",
    "print(\"Axes columns:\", [\"IE\", \"NS\", \"TF\", \"JP\", \"IE_y\", \"NS_y\", \"TF_y\", \"JP_y\"])\n",
    "print(\"Continuous trait columns:\", ['extraversion_score','sensing_score','thinking_score','judging_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Comparing 200 docs:\n",
      "\n",
      "Comparing 200 docs:\n",
      "\n",
      "  repr  dim  train_acc  val_acc\n",
      "TF-IDF   30   0.478571 0.533333\n",
      "   W2V   30   0.257143 0.216667\n",
      "  BERT  384   0.985714 0.250000\n",
      "\n",
      "Done in 4.0s\n",
      "  repr  dim  train_acc  val_acc\n",
      "TF-IDF   30   0.478571 0.533333\n",
      "   W2V   30   0.257143 0.216667\n",
      "  BERT  384   0.985714 0.250000\n",
      "\n",
      "Done in 4.0s\n"
     ]
    }
   ],
   "source": [
    "### TASK 2B: NLP Feature Engineering (Full + Optimized Implementation) ###\n",
    "# Objective: Generate text features using multiple approaches —\n",
    "# (1) TF-IDF vectors, (2) (GloVe), and (3) contextual BERT embeddings.\n",
    "# Then compare their representational quality and computational trade-offs.\n",
    "%pip install -q sentence-transformers gensim\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import re, time, numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# load mbti if needed (fast path)\n",
    "if 'mbti' not in globals():\n",
    "    try:\n",
    "        mbti = pd.read_csv(\"dataset/myer-briggs-data.csv\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"MBTI data not found: {e}\")\n",
    "\n",
    "# ultra-fast cleaner (minimal regex)\n",
    "def clean_text(s):\n",
    "    s = str(s).lower()\n",
    "    return ' '.join(w for w in re.sub(r'[^a-z0-9\\s]',' ',s).split() if len(w) > 1)\n",
    "\n",
    "# ensure cleaned text exists\n",
    "if 'cleaned_text' not in mbti.columns or not mbti['cleaned_text'].astype(bool).sum():\n",
    "    if 'posts' in mbti.columns:\n",
    "        mbti['cleaned_text'] = mbti['posts'].fillna('').apply(clean_text)\n",
    "    else:\n",
    "        cols = [c for c in mbti.columns if any(k in c.lower() for k in ('post','text'))]\n",
    "        mbti['cleaned_text'] = mbti[cols[0] if cols else mbti.columns[0]].fillna('').apply(clean_text)\n",
    "\n",
    "# tiny sample for ultra-fast demo\n",
    "n = min(200, len(mbti))\n",
    "texts = mbti['cleaned_text'].iloc[:n].tolist()\n",
    "labels = mbti['type'].iloc[:n].fillna('UNK').tolist() if 'type' in mbti.columns else ['UNK'] * n\n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "print(f\"Comparing {n} docs:\\n\")\n",
    "\n",
    "# 1) TF-IDF + tiny SVD\n",
    "X_tfidf_svd = TruncatedSVD(n_components=30, random_state=42).fit_transform(\n",
    "    TfidfVectorizer(max_features=1000).fit_transform(texts)\n",
    ")\n",
    "\n",
    "# 2) Fast Word2Vec or fallback\n",
    "tokens = [t.split() for t in texts]\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    w2v = Word2Vec(sentences=tokens, vector_size=30, window=3, min_count=2, workers=2, epochs=5)\n",
    "    X_w2v = np.vstack([\n",
    "        np.mean([w2v.wv[w] for w in t if w in w2v.wv], axis=0) if any(w in w2v.wv for w in t)\n",
    "        else np.zeros(30) for t in tokens\n",
    "    ])\n",
    "except Exception:\n",
    "    rng = np.random.default_rng(42)\n",
    "    vocab = {w for t in tokens for w in t}\n",
    "    sim = {w: rng.normal(size=30) for w in vocab}\n",
    "    X_w2v = np.vstack([np.mean([sim[w] for w in t], axis=0) if t else np.zeros(30) for t in tokens])\n",
    "\n",
    "# 3) SentenceTransformer (small model) or ngram fallback\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    X_bert = SentenceTransformer('paraphrase-MiniLM-L3-v2').encode(\n",
    "        texts, show_progress_bar=False, batch_size=32\n",
    "    )\n",
    "except Exception:\n",
    "    X_bert = TruncatedSVD(n_components=30).fit_transform(\n",
    "        TfidfVectorizer(max_features=1000, ngram_range=(1,2)).fit_transform(texts)\n",
    "    )\n",
    "\n",
    "# eval with train & validation accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def eval_acc(X):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    val_acc = clf.score(X_val, y_val)\n",
    "    return train_acc, val_acc\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {'repr': k, 'dim': X.shape[1], 'train_acc': ta, 'val_acc': va}\n",
    "    for k, X in [('TF-IDF', X_tfidf_svd), ('W2V', X_w2v), ('BERT', X_bert)]\n",
    "    for ta, va in [eval_acc(X)]\n",
    "])\n",
    "print(results.to_string(index=False))\n",
    "print(f\"\\nDone in {time.time()-start:.1f}s\")\n",
    "\n",
    "#repr: representation method\n",
    "    # What it is: text embedding technique used - TF-IDF, Word2Vec (W2V), or BERT\n",
    "    # How it helps: Identifies which NLP approach generated the features so you can compare them side-by-side\n",
    "\n",
    "#dim: feature dimensionality\n",
    "    # What it is: Number of features/dimensions in each representation\n",
    "        # TF-IDF: 30 (after SVD compression)\n",
    "        # W2V: 30 (vector size)\n",
    "        # BERT: 384 (model's native embedding size)\n",
    "    # How it helps: Shows the compactness of each representation; lower dimensions often mean faster training\n",
    "\n",
    "#train_acc: training accuracy\n",
    "    # What it is: Percentage of MBTI types correctly predicted on the 70% training data (the data the model learned from)\n",
    "   \n",
    "    # How it helps: Shows how well each representation can fit patterns in the data\n",
    "        # High train_acc means the embedding captures learnable patterns\n",
    "        # Warning sign: If train_acc is very high but val_acc is low, the model is overfitting\n",
    "\n",
    "# val_acc: validation accuracy\n",
    "    # What it is: Percentage of MBTI types correctly predicted on the 30% held-out validation data (unseen data)\n",
    "    # How it helps with the task: This is the most important metric for comparing \"representational quality\"\n",
    "        # Shows which embedding generalizes best to new data\n",
    "        # Higher val_acc = better representation for predicting personality types\n",
    "        # Directly measures which method (TF-IDF/Word2Vec/BERT) best captures the text patterns that distinguish MBTI types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "caricon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
